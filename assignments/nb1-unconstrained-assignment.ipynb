{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd8ea3a",
   "metadata": {},
   "source": [
    "# A1: Unconstrained Optimization\n",
    "\n",
    "---\n",
    "\n",
    "*Purpose*: The simplest kind of optimization problem is one without constraints---an *unconstrained optimization* problem. We will begin our study of optimization with this simple setting.\n",
    "\n",
    "*Learning Objectives*:\n",
    "- build and explore models with Grama\n",
    "- learn the first-order necessary condition (FONC) and curvature condition\n",
    "- use `gr.ev_min()` to optimize unconstrained problems with multiple minima\n",
    "- apply these concepts to the context of Nonlinear Least Squares (NLS) model-fitting\n",
    "\n",
    "*Reading*:\n",
    "- Grama documentation, [verbs](https://py-grama.readthedocs.io/en/latest/source/language.html#verbs)\n",
    "- del Rosario and Iaccarino (2024), Ch. 3 (Modeling process)\n",
    "- Kochenderfer and Wheeler (2019), Ch. 1 (Intro to optimization), Ch. (Algorithms---skim only)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c96aba",
   "metadata": {},
   "source": [
    "### Assignment Checklist\n",
    "\n",
    "1. [?] Make sure you have answered all questions. These are marked with a **qX.Y**\n",
    "1. [?] Make sure you complete the Project Task at the end of the assignment. These will scaffold your project progress during the semester.\n",
    "1. [?] Make sure your notebook passes all `assert()` statements. You will not get full credit for the assignment if a single `assert()` fails.\n",
    "1. [?] Make sure your notebook runs: `Kernel > Restart kernel and run all cells...`\n",
    "1. [?] Upload your notebook to Canvas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee390bdc",
   "metadata": {},
   "source": [
    "### Grading Rubric\n",
    "\n",
    "Every assignment is worth 10 points; it is not possible to receive less than 0 points. For each question (qX.Y) on a given assignment, the following grading rubric will be applied. For every NI that you receive, one point will be subtracted from your assignment total. For reference, to receive an A- in this class, you will need an average of 9 points across your 5 best assignments, meaning you need to have at most one mistake on your final submission for 5 assignments. To achieve this, you should take advantage of both the Draft and Final submission deadlines.\n",
    "\n",
    "| Category     | Needs Improvement (NI)                     | Satisfactory (S)                       |\n",
    "|--------------|--------------------------------------------|----------------------------------------|\n",
    "| Effort       | qX.Y left unattempted                      | qX.Y attempted                         |\n",
    "| Assertions   | Code does not pass an `assert()`           | All `assert()`s pass, or no assertions |\n",
    "| Observations | Any point under *observe* left unattempted | All *observe*s attempted and correct,  |\n",
    "|              | Provided an incorrect observation          | or no *observe*s for that q            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16d23d",
   "metadata": {},
   "source": [
    "## S1: Grama Model Implementation\n",
    "\n",
    "---\n",
    "\n",
    "[Grama](https://joss.theoj.org/papers/10.21105/joss.02462) is an [open-source](https://github.com/zdelrosario/py_grama) Python package designed to support model analysis with quantified uncertainties. We will use it throughout the course to implement and analyze models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332157f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grama as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "DF = gr.Intention()\n",
    "\n",
    "# Set figure options\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17417b4d",
   "metadata": {},
   "source": [
    "Using Grama consists of two steps:\n",
    "\n",
    "1. **Implement** the model, which consists of functions, bounds, and densities to describe the uncertainty\n",
    "1. **Analyze** the model, which can involve optimization, inference, curve-fitting, uncertainty propagation, sensitivity analysis, etc.\n",
    "\n",
    "To do analysis, we first have to implement the model. The following code demonstrates how to implement a simple model with Grama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit; the following demonstrates how to implement a Grama model\n",
    "\n",
    "# Define the function\n",
    "def fun_example(x, y, z):\n",
    "    # Perform computation\n",
    "    return (x + y) ** z\n",
    "# These must match the order used above!\n",
    "var_example = [\"x\", \"y\", \"z\"]\n",
    "# Name the output(s) of the function\n",
    "out_example = [\"f\"]\n",
    "\n",
    "# Grama syntax is a little strange, but is designed to promote readability of code\n",
    "# using *functional programming patterns* and pipes `>>`\n",
    "md_example = (\n",
    "    gr.Model(\"Example\")\n",
    "    >> gr.cp_function(\n",
    "        fun=fun_example, # Specify the function\n",
    "        var=var_example, # Name the input(s)\n",
    "        out=out_example, # name the output(s)\n",
    "    )\n",
    "    >> gr.cp_bounds(\n",
    "        x=(-1, +1),\n",
    "        y=(-1, +1),\n",
    "        z=(0.5, 2.0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Once built, we can print a helpful summary of a Grama model\n",
    "md_example.printpretty()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1bea6",
   "metadata": {},
   "source": [
    "The important parts of the code above:\n",
    "\n",
    "> `gr.Model()` creates a blank Grama model\n",
    "\n",
    "This is the starting point for creating a model. We can add things like functions, bounds on inputs, or a description of uncertainty to a model by using `comp`osition verbs.\n",
    "\n",
    "> `gr.comp_function(md, ...)` adds a function to a Grama model\n",
    "\n",
    "A function maps inputs to outputs; Grama models can have multiple functions.\n",
    "\n",
    "> `>>` is the *pipe operator* in Grama\n",
    "\n",
    "The pipe operator `>>` takes the left-hand-side and inserts it as the first argument into the right-hand-side function. So `gr.comp_function(md, ...)` can be written as `md >> gr.cp_function(...)`. We shorten the prefix `comp` to `cp` to tell Grama to use the pipe-enabled version of the verb.\n",
    "\n",
    "> `gr.comp_bounds(...)` adds bounds for inputs to a Grama model\n",
    "\n",
    "These are useful for specifying optimization bounds and setting nominal variable conditions.\n",
    "\n",
    "> By specifying both *functions* and the *input space*, Grama models make sophisticated analyses very simple.\n",
    "\n",
    "You'll get some practice using these Grama tools below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d036a",
   "metadata": {},
   "source": [
    "### q1\\.1 Add a function\n",
    "\n",
    "Implement the following expression as a function, and add it to the grama model below.\n",
    "\n",
    "$$f(x, y, z) = z \\cdot \\exp(x + y).$$\n",
    "\n",
    "*Hint*: Feel free to pattern-match using the example code given above. If you need more direction for this, you can [read the documentation](https://py-grama.readthedocs.io/en/latest/source/language.html#model-building) on model building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Implement the function above, add it to md_task below\n",
    "###\n",
    "\n",
    "# TASK: Uncomment and fill-in the `task` quantities below\n",
    "# def fun_task(X):\n",
    "#     pass\n",
    "# var_task = []\n",
    "# out_task = []\n",
    "\n",
    "\n",
    "# TASK: Add fun_task as a function to this model\n",
    "md_task = (\n",
    "    gr.Model(\"Task model\")\n",
    "## YOUR CODE HERE\n",
    "\n",
    ")\n",
    "\n",
    "# NOTE: No need to edit this; use this to check your work\n",
    "md_task.printpretty()\n",
    "df_task_res = gr.eval_df(\n",
    "    md_task,\n",
    "    df=gr.df_make(x=+1, y=-1, z=+2)\n",
    ")\n",
    "assert(df_task_res[[\"f\"]].equals(gr.df_make(f=2.0)))\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abe3c5",
   "metadata": {},
   "source": [
    "### q1\\.2 Add bounds\n",
    "\n",
    "Add bounds from $[-1, +1]$ for `x, y, z` to the model `md_task`. If you do this correctly, the following code will produce a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Add bounds to x, y, z in md_task\n",
    "###\n",
    "\n",
    "# TASK: Add bounds to this model\n",
    "md_task = (\n",
    "    md_task\n",
    "# YOUR CODE HERE\n",
    "\n",
    ")\n",
    "\n",
    "# NOTE: No need to edit; this will construct \"ceteris paribus\" plots\n",
    "#       for your model\n",
    "(\n",
    "    md_task\n",
    "    >> gr.ev_sinews(df_det=\"swp\", n_sweeps=5, n_density=20, seed=101)\n",
    "    >> gr.pt_auto()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f00fa",
   "metadata": {},
   "source": [
    "Once you've build a Grama model, you can use Grama verbs to carry out very sophisticated analyses with few lines of code. For instance, the code above generates parameter sweep plots for your model, which economists and machine learning researchers use to understand the effects of variables on a model. In the Case Study below we'll see how we can use Grama to do curve fitting as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e060cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## S2: Tenchi Diagrams: The System Boundary\n",
    "\n",
    "---\n",
    "\n",
    "As engineers, we don't do optimization just for fun, we use optimization to help make decisions about *systems*. To do this, we need to first *define the system* we're interested in. One of the first and most important steps in defining a system is to draw the *system boundary*. This boundary defines what we're interested in studying (what's inside the system), and what lies outside the scope of our study (what's outside the system).\n",
    "\n",
    "As an example, let's return to a system we studied all the way back in ModSim (Worksheet 3): Phillips' study of HIV infection dynamics. Figure 1 depicts the stock-and-flow model he used, and a system boundary defining what the model represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333aff77",
   "metadata": {},
   "source": [
    "<img src=\"./images/tenchi-phillips1996.png\" width=\"400\">\n",
    "\n",
    "**Figure 1.** Tenchi diagram of a model for HIV infection dynamics. Notice how only certain cell types (CD4 Lymphocytes) and virions are included in the system boundary (dashed line). Other cell types (red blood cells) are not included, and hence not tracked in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f659404",
   "metadata": {},
   "source": [
    "Notice that Phillips' model represents a *tiny* fraction of the body: only 1 mm^3 of blood! Furthemore, the system boundary \"squiggles\" to avoid parts of the body Phillips doesn't want to study. The model includes CD4 Lymphocites (a type of white blood cell), but excludes red blood cells. Notice that CD4 Lymphocytes are represented by variables in the model (R, L, E), but red blod cells are not represented in the model.\n",
    "\n",
    "Drawing the system boundary is an important source of *model assumptions*. We often have to make assumptions to represent *interactions* between the system and its environment; that is, whenever things *cross the system boundary*. That's what the $\\Gamma\\tau$ term in Phillips' model represents: additional CD4 Lymphocytes produced by the body which enter our small volume of blood.\n",
    "\n",
    "Even when something is inside the system, we must make assumptions about how we will *represent* it. For example, Phillips' model tracks the number of free viral particles *only* in terms of count $V$. Real virions tend to decay over time, so each viral particle will have its own lifetime. However, Phillips decided to represent virion death using a steady rate ($\\sigma$) that does not account for individual lifetimes.\n",
    "\n",
    "Finally, when doing optimization, we make additional choices about howe we interact with the system. We will choose a set of *decision variables* that we seek to modify, and leave all other quantities as fixed *parameters*. The \"with respect to\" (wrt) line of an optimization problem specifies which quantities are decision variables; everything else in the model is treated as a fixed parameter *while we solve the optimization problem*. However, just because something is a parameter doesn't mean we don't have influence over it.\n",
    "\n",
    "Next, let's return to the simple model for the Arecibo Observatory that we saw in NB0 (Fig. 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29acf4",
   "metadata": {},
   "source": [
    "<img src=\"./images/tenchi-springoverhead.png\" width=\"400\">\n",
    "\n",
    "**Figure 2.** Tenchi diagram of our simple model for the Arecibo Observatory, top view.\n",
    "\n",
    "We also had a model for the location $\\vec{x}\\in\\mathbb{R}^2$ of the suspended mass, given the anchor locations $\\vec{v}_i\\in\\mathbb{R}^2$ and spring constants $k_i$.\n",
    "\n",
    "$$\\min E(\\vec{x}) = \\frac{1}{2}\\left(K_1 \\|\\vec{x} - \\vec{v}_1\\|_2^2 + K_2 \\|\\vec{x} - \\vec{v}_2\\|_2^2 + K_3 \\|\\vec{x} - \\vec{v}_3\\|_2^2\\right)$$\n",
    "$$\\text{wrt} \\,\\vec{x}$$\n",
    "\n",
    "You'll study this system in the following task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3728bd",
   "metadata": {},
   "source": [
    "### q2\\.1 Study the system boundary\n",
    "\n",
    "Answer the following questions.\n",
    "\n",
    "*Hint*: Remember that we were using optimization to *solve* a physics problem. So in this case, the \"decision variables\" are not so much things we're tweaking in design, but unknown quantities that we're trying to solve for. \n",
    "\n",
    "*Observe*:\n",
    "\n",
    "- Is the suspended mass represented in the model? If yes, what quantity represents it, and how is it represented?\n",
    "  - (Your response here)\n",
    "- Are the towers represented in the model? If yes, what quantities represent them, and how do they represent them?\n",
    "  - (Your response here)\n",
    "- Are the main cables represented in the model? If yes, what quantities represent them, and how do they represent them?\n",
    "  - (Your response here)\n",
    "- Are the backstays (supporting cables) represented in the model? If yes, what quantities represent them, and how do they represent them?\n",
    "  - (Your response here)\n",
    "- What are our decision variables?\n",
    "  - (Your response here)\n",
    "- Given your answers to the previous questions, can we determine how tower placement affects suspended mass location?\n",
    "  - (Your response here)\n",
    "- Given your answers to the previous questions, can we determine how strong the towers need to be?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78da69",
   "metadata": {},
   "source": [
    "### What's in the system? What's not?\n",
    "\n",
    "Drawing the system boundary is a key first step to building a model. However, it's also important to maintain some awareness of what we're *not* modeling---what's outside the system we've defined. \n",
    "\n",
    "For instance: Nowhere in our tenchi diagram (Fig. 2) did we depict *wind*---neither in the natural world nor in the model. However, Arecibo is in Puerto Rico, which is subject to severe storms. Also, nowhere did we depict ongoing activities like maintenence. The Arecibo Observatory actually collapsed in late 2020, in [large part](https://doi.org/10.17226/26982) due to damage from storm winds and lack of proper maintenence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9920c",
   "metadata": {},
   "source": [
    "## S3: Optimization: Necessary Conditions\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we'll discuss the necessary conditions for *minimizing* a function. We'll also take a look at the workhorse optimization routine `scipy.optimize.minimize`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f966609",
   "metadata": {},
   "source": [
    "### *Aside*: What about Maximization?\n",
    "\n",
    "Maximizing an objective function can be trivially reformulated as minimization. If we seek to maximize a function\n",
    "\n",
    "$$\\text{max}\\, y(x)$$\n",
    "$$\\text{wrt.}\\, x$$\n",
    "\n",
    "Then we can formulate it as\n",
    "\n",
    "$$\\text{min}\\, -y(x)$$\n",
    "$$\\text{wrt.}\\, x$$\n",
    "\n",
    "For this reason `scipy` only implements a `minimize` routine; you'll need to negate your objective if you want to do maximization!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73937d3",
   "metadata": {},
   "source": [
    "## Running Example\n",
    "\n",
    "For this section we'll consider a running example:\n",
    "\n",
    "$$f = \\sin(3/2 \\pi x) + x^2$$\n",
    "\n",
    "The following code implements this function and visualizes it across an interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11799c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit, this code sets up the objective function\n",
    "fun_multi = lambda x: np.sin(1.5 * np.pi * x) + x**2\n",
    "md_multi = (\n",
    "    gr.Model() \n",
    "    >> gr.cp_function(\n",
    "        # NOTE: `lambda` is the \"inline function definition\" keyword;\n",
    "        #       this is just a shortcut to help us define a function\n",
    "        fun=fun_multi,\n",
    "        var=[\"x\"],\n",
    "        out=[\"f\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compute data and visualize\n",
    "(\n",
    "    md_multi\n",
    "    >> gr.ev_df(df=gr.df_make(x=gr.linspace(-3, +3, n=200)))\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"f\"))\n",
    "    + gr.geom_line()\n",
    "    + gr.labs(x=\"x\", y=\"Objective\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e747aed",
   "metadata": {},
   "source": [
    "## First-Order Necessary Condition (FONC)\n",
    "\n",
    "The first-order necessary condition for optimality is that the gradient of the objective function equals zero, that is $\\nabla f(x) = 0$.\n",
    "\n",
    "As we'll see below, this alone is a *necessary*, but *not sufficient* condition for optimality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f1a48",
   "metadata": {},
   "source": [
    "### q3\\.1 Compute the derivative to apply the FONC\n",
    "\n",
    "Derive the gradient of the function above and answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Derive the gradient (derivative) of the function above,\n",
    "#       implement it as a function.\n",
    "###\n",
    "\n",
    "# fun_prime = lambda x: ???\n",
    "\n",
    "\n",
    "# NOTE: No need to edit, this will visualize your result\n",
    "# Apply FONC via root-finding routine (Newton's method)\n",
    "X_roots = list(map(\n",
    "    lambda x0: sp.optimize.newton(fun_prime, x0),\n",
    "    [-1.5, -1, -0.5, 0.5, 1, 1.5, 2.1]\n",
    "))\n",
    "F_roots = list(map(fun_multi, X_roots))\n",
    "\n",
    "(\n",
    "    md_multi\n",
    "    >> gr.ev_df(df=gr.df_make(x=gr.linspace(-3, +3, n=200)))\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"f\"))\n",
    "    + gr.geom_point(data=gr.df_make(x=X_roots, f=F_roots))\n",
    "    + gr.geom_line()\n",
    "    + gr.labs(x=\"x\", y=\"Objective\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc47394",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- Does every point satisfying the FONC correspond to a \"low\" point on the objective function? (yes or no) \n",
    "  - (Your response here)\n",
    "- Provide an example of a point `x` with $f'(x) = 0$ that is not a \"low\" point on the function.\n",
    "  - (Your response here)\n",
    "- How does the curvature of the function relate to whether a point is a minimum or a maximum?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c8964",
   "metadata": {},
   "source": [
    "## Positive Curvature\n",
    "\n",
    "The other necessary condition for minimality is that the hessian of the objective function (its second-derivative matrix) is [positive-definite](https://en.wikipedia.org/wiki/Definite_symmetric_matrix). This condition is a higher-dimensional analogue to having positive curvature, and it is denoted mathematically by $\\nabla^2 f \\succ 0$ (note $\\succ$ rather than $>$).\n",
    "\n",
    "The Hessian is the matrix of mixed second-order partial derivatives, that is\n",
    "\n",
    "$$[\\nabla^2 f]_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}.$$\n",
    "\n",
    "In the case where $x$ is one-dimensional (where there is only one scalar value for curvature), the hessian reduces to the familiar second derivative. The higher-dimensional case of the SONC essentially demands that \"all\" of the curvatures (in \"every dimension\") must be positive.\n",
    "\n",
    "In order for a point to be a \"low point\", it needs to satisfy both the FONC and have positive curvature. These together are *sufficient* conditions for a point to be what we call a *local minimum*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86be75",
   "metadata": {},
   "source": [
    "### q3\\.2 Compute the 2nd derivative to apply the positive curvature condition\n",
    "\n",
    "Derive the second derivative of the function above, implement it, and answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Derive the second derivative of the function above,\n",
    "#       implement it as a function.\n",
    "###\n",
    "\n",
    "# fun_curvature = lambda x: ???\n",
    "\n",
    "\n",
    "# NOTE: No need to edit, this will visualize your result\n",
    "# Apply curvature check to filter points\n",
    "X_min = list(filter(\n",
    "    lambda x: fun_curvature(x) > 0, \n",
    "    X_roots\n",
    "))\n",
    "F_min = list(map(fun_multi, X_min))\n",
    "\n",
    "(\n",
    "    md_multi\n",
    "    >> gr.ev_df(df=gr.df_make(x=gr.linspace(-3, +3, n=200)))\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"f\"))\n",
    "    + gr.geom_point(data=gr.df_make(x=X_min, f=F_min))\n",
    "    + gr.geom_line()\n",
    "    + gr.labs(x=\"x\", y=\"Objective\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e428f6",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- Do each of the local minima (visualized as dots above) have the same Objective function value? (yes or no)\n",
    "  - (Your response here)\n",
    "- Is there one local minimum that seems to have the *smallest* Objective function value? If yes, roughly what $x$ value achieves that smallest value?\n",
    "  - (Your response here)\n",
    "- Are the FONC and positive curvature conditions together *sufficient* to guarantee the smallest objective function value (as opposed to a \"locally-smallest\" value)? (yes or no)\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386857f1",
   "metadata": {},
   "source": [
    "### Some key terminology\n",
    "\n",
    "Given a function $f(x)$:\n",
    "\n",
    "- A *minimizer* of a function is a point $x^*$ that satisfies both the FONC $\\nabla f(x^*) = 0$ and positive curvature $\\nabla^2 f(x^*) \\succ 0$.\n",
    "- A *minimum* of a function is a function value $f^*=f(x^*)$ associated with a minimizer $x^*$.\n",
    "\n",
    "Keep in mind that a *minimum* is a function value, while a *minimizer* is a point in the function's domain; they'll generally have different units and different dimensionality!\n",
    "\n",
    "- A *global minimum* is the smallest value $f^*$ that $f(x)$ takes over its entire domain; functions that are *unbounded below* have a global minimum of $-\\infty$. \n",
    "  - Similarly a *global minimizer* is a point $x^*$ that achieves the global minimum value. \n",
    "- A *local minimum* is any function value $f^*$ where the FONC and positive curvature conditions are met; a local minimum can also be the global minimum. \n",
    "  - Similarly a *local minimizer* is any point $x^*$ that achieves a local minimum value. There can be multiple, distinct *global minimizers*, but only one *global minimum*.\n",
    "\n",
    "Full disclosure: These are *formal definitions* from optimization theory. While it's useful to know these definitions, we generally can't use them alone to solve practical optimization problems. Instead, we use an *optimization algorithm*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dae738",
   "metadata": {},
   "source": [
    "## Iterative Optimization Algorithms\n",
    "\n",
    "In this class we'll primarily talk about *using* optimization, not *developing optimization algorithms*. However, it's important to understand some basics about how most optimization algorithms work.\n",
    "\n",
    "Most **iterative optimization** algorithms start with an initial guess $x_0$ and attempt to \"improve\" the function value $f(x_i)$ by searching for a new point $x_{i+1}$ where the value is lower $f(x_{i+1}) < f(x_i)$. The most intuitive approach is [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), where we \"follow\" the function gradient downhill ($-\\nabla f$) until we stop moving. There is a veritable zoo of optimization algorithms that are much more efficient than gradient descent, but most tend to start with an initial guess and \"run downhill\".\n",
    "\n",
    "The following code runs `scipy.optimize.minimize` on our running example `fun_multi`. The `minimize` routine takes a function to optimize $f$ and an initial guess $x_0$. It then \"marches\" towards a local minimum of the function. Note that `minimize` returns an output *object* (call it `res`), and that we can access its minimizer with `res.x` and minimum with `res.fun`.\n",
    "\n",
    "Given what we've seen above, it should make sense that **the inital guess strongly affects where iterative optimization ends**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ed84c",
   "metadata": {},
   "source": [
    "### q3\\.3 How does an optimization algorithm behave?\n",
    "\n",
    "The following code picks a set of inital guesses $x_0$ and runs `minimize` for each one. Run the following code and answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit, run and inspect\n",
    "# Run optimizer with several restarts\n",
    "df_hist = pd.DataFrame()\n",
    "x_starts = [-3, -2, -0.9, 1.5, 3]\n",
    "for i, x in enumerate(x_starts):\n",
    "    # Setup callback for storing history\n",
    "    x_hist = [x]\n",
    "    f_hist = [fun_multi(x)]\n",
    "    def callback(Xi):\n",
    "        global x_hist, f_hist\n",
    "        x_hist.append(Xi[0])\n",
    "        f_hist.append(fun_multi(Xi)[0])\n",
    "        \n",
    "    # Minimize\n",
    "    res = sp.optimize.minimize(fun_multi, x, callback=callback)\n",
    "    xs = res.x\n",
    "    fs = res.fun\n",
    "    \n",
    "    # Store results\n",
    "    df_tmp = gr.df_make(x=x_hist, f=f_hist, i=i)\n",
    "    df_hist = pd.concat((df_hist, df_tmp), axis=0)\n",
    "\n",
    "# Visualize\n",
    "(\n",
    "    md_multi\n",
    "    >> gr.ev_df(df=gr.df_make(x=gr.linspace(-3, +3, n=200)))\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"f\"))\n",
    "    + gr.geom_line()\n",
    "    + gr.geom_point(data=df_hist, mapping=gr.aes(color=\"factor(i)\"))\n",
    "    + gr.geom_line(data=df_hist, mapping=gr.aes(color=\"factor(i)\"))\n",
    "    + gr.scale_color_discrete(name=\"Iteration\")\n",
    "    + gr.labs(x=\"x\", y=\"Objective\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648b8cc",
   "metadata": {},
   "source": [
    "Every distinct color represents the optimization *trajectory* (set of $x_i$) of a distinct initial guess.\n",
    "\n",
    "*Observe*:\n",
    "\n",
    "- Does every initial guess arrive at the same minimizer? (yes or no)\n",
    "  - (Your response here)\n",
    "- Does every initial guess arrive at it nearest minimum? (yes or no)\n",
    "  - (Your response here)\n",
    "- Do different initial guesses **sometimes** arrive at the same local minimum? (yes or no)\n",
    "  - (Your response here)\n",
    "- In this case we are able to *visually identify* which minimum is the global minimum. Imagine we had a function with 100 input variables. Would we be able to use the same approach to *verify* a given local minimum is the global minimum? (yes or no) Why or why not?\n",
    "  - (Your response here)\n",
    "  - (Support your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379d859",
   "metadata": {},
   "source": [
    "These results suggest that we need to use iterative optimization algorithms with *caution*. The following heuristic is a good rule-of-thumb:\n",
    "\n",
    "**Heuristic**: When seeking to minimize a function with an iterative algorithm, it is a good idea to test **multiple starting values** $x_0$ and check whether they arrive at the same minimizer $x^*$. The more trajectories that terminate at the same minimizer $x^*$ (with the smallest observed minimum value), the more credibility that point has as the possible global minimum of the objective function.\n",
    "\n",
    "If all points arrive at the same minimizer $x^*$ then there is *no evidence* for local minima: Note that this does **not** prove that there are no local minima, only that we don't have evidence for other minima!\n",
    "\n",
    "If distinct starting points arrive at different minimizers $x^*$ then we have clear evidence for the presence of local minima. Depending on your application you might be ok with this outcome. If multiple minima are unacceptable, it is sometimes possible to *reformulate* the problem to remove multiple minima; we'll see an example of this in the case study below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c29490",
   "metadata": {},
   "source": [
    "### q3\\.4 Optimizing a function\n",
    "\n",
    "Use `gr.ev_min()` to search for the global minimum of the following function. Document your results from multiple restarts (at least ten).\n",
    "\n",
    "*Hint*: Keep in mind that in Jupyter you can look up the documentation for a function by placing your cursor on the function and pressing `Shift + Tab`. This is useful for looking up the arguments of a function and understanding the structure of its output.\n",
    "\n",
    "*Aside*: The function `gr.ev_min()` calls optimization routines from Scipy. You can look for the documentation for `scipy.optimize.minimize` for more details on the optimization routines available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Find the minimum of the provided function\n",
    "###\n",
    "\n",
    "# NOTE: No need to edit; find the minimum of this function (implemented as a grama model)\n",
    "def fun_2d(x, y):\n",
    "    \"\"\"Levi's function\"\"\"\n",
    "    return np.sin(3 * np.pi * x)**2 + \\\n",
    "        (x - 1)**2 * (1 + np.sin(3 * np.pi * y)**2) + \\\n",
    "        (y - 1)**2 * (1 + np.sin(2 * np.pi * y)**2)\n",
    "\n",
    "md_2d = (\n",
    "    gr.Model()\n",
    "    >> gr.cp_function(\n",
    "        fun=fun_2d,\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"f\"],\n",
    "    )\n",
    "    >> gr.cp_bounds(x=[0.6, 1.6], y=[0.6, 1.6])\n",
    ")\n",
    "\n",
    "# TASK: Use gr.ev_min() to search for the minimum of the function\n",
    "# df_2d = ???\n",
    "\n",
    "\n",
    "# NOTE: Do not edit, your code should pass these checks\n",
    "assert df_2d.shape[0] >= 10, \"You must use at least ten restarts\"\n",
    "# NOTE: No need to edit; this visualizes your result\n",
    "df_2d = df_2d >> gr.tf_arrange(DF.f)\n",
    "print(\"Results of all restarts:\")\n",
    "print(df_2d)\n",
    "print(\"Best result:\")\n",
    "print(\"  optimizer: {}, {}\".format(df_2d.x[0], df_2d.y[0]))\n",
    "print(\"  above global minimum: {}\".format(df_2d.f[0]))\n",
    "(\n",
    "    md_2d\n",
    "    >> gr.ev_contour(\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"f\"],\n",
    "        n_side=100,\n",
    "    )\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"y\"))\n",
    "    + gr.geom_segment(gr.aes(xend=\"x_end\", yend=\"y_end\", group=\"level\", color=\"level\"))\n",
    "    + gr.geom_point(data=df_2d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca3c3f",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- How far is your identified minimum above the global minimum? Is it greater than zero? (See the text printout above.)\n",
    "  - (Your response here)\n",
    "- Aside from the \"above the minimum\" value reported (which you would *not* have in practice), what evidence do you have that your minimum is the global minimum?  Does this *prove* that your minimum is the global minimum?\n",
    "  - (Your response here)\n",
    "  - (Assess the strength of your evidence here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530362f3",
   "metadata": {},
   "source": [
    "Multiple minima is not some abstract issue; it shows up in **real** engineering problems, as we'll see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a4e11",
   "metadata": {},
   "source": [
    "## S4: Case Study: Least Squares Regression\n",
    "\n",
    "---\n",
    "\n",
    "Often we have some model for a physical system with *unknown parameters*. We can combine data with [least squares regression](https://en.wikipedia.org/wiki/Non-linear_least_squares) to *fit* the model---that is, optimize the agreement of the model with data to find optimal parameter values. This is a very common use of optimization, but the issue of multiple minima can make model fitting very difficult.\n",
    "\n",
    "To illustrate, let's take a look at a case study involving the frequency response of a circuit. An LC circuit starting with initial voltage difference $V_0$ has the following current waveform:\n",
    "\n",
    "$$i_{\\text{true}}(t, \\theta) = \\sqrt{C/L} V_0 \\sin(\\sqrt{1/(LC)} \\cdot t)$$\n",
    "\n",
    "where $\\theta = [V_0, C, L]$ is the vector of *parameters* for the model. We further assume that the true current value is corrupted by additive noise:\n",
    "\n",
    "$$i_{\\text{meas}}(t, \\theta) = i_{\\text{true}}(t) + \\epsilon_{\\text{noise}}$$\n",
    "\n",
    "The following code plots this wave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit; this creates and visualizes our data\n",
    "np.random.seed(101)\n",
    "n = 100\n",
    "df_data = (\n",
    "    gr.df_make(t=np.linspace(0, 5, num=n))\n",
    "    >> gr.tf_mutate(i_true=20 * gr.sin(2 * DF.t))\n",
    "    >> gr.tf_mutate(\n",
    "        i=DF.i_true + np.random.normal(size=n, scale=0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "(\n",
    "    df_data\n",
    "    >> gr.ggplot(gr.aes(\"t\", \"i\"))\n",
    "    + gr.geom_point()\n",
    "    + gr.labs(x=\"Time (s)\", y=\"Current (Amps)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d245d",
   "metadata": {},
   "source": [
    "### Application of NLS to Circuit Analysis\n",
    "\n",
    "The expression $i_{\\text{true}}(t, \\theta)$ above is a parameterized function that we can fit with [Non-linear least squares](https://en.wikipedia.org/wiki/Non-linear_least_squares) (NLS). NLS formulates an objective function for this fitting based on the *mean squared error* (MSE)\n",
    "\n",
    "$$\\text{MSE}(\\hat{\\theta}) = \\frac{1}{n}\\sum_{i=1}^n (i_{\\text{true}}(t_i, \\hat{\\theta}) - i_{\\text{measured}}(t_i, \\theta_{\\text{true}}) )^2$$\n",
    "\n",
    "Our assumption in NLS is that there exist some \"true\" parameter values $\\theta_{\\text{true}}$, and we seek to optimize $\\hat{\\theta}$ so as to minimize the MSE and recover the true parameters $\\theta_{\\text{true}}$. **Importantly** unless there is special structure in our parameterized function, there is no guarantee that there exists a single \"best\" parameter value $\\hat{\\theta}^*$ that minimizes the MSE. This can lead to multiple minima, as we'll see below.\n",
    "\n",
    "Given all this, we can formulate NLS as an optimization problem:\n",
    "\n",
    "$$\\min\\, \\text{MSE}(\\hat{\\theta})$$\n",
    "$$\\text{wrt.}\\, \\hat{\\theta}$$\n",
    "\n",
    "In this case study we'll apply NLS to the circuit waveform. We'll break this down into steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188db1c",
   "metadata": {},
   "source": [
    "### q4\\.1 Implement the signal function\n",
    "\n",
    "Implement the function\n",
    "\n",
    "$$i_{\\text{true}}(t, \\theta) = \\sqrt{C/L} V_0 \\sin(\\sqrt{1/(LC)} \\cdot t)$$\n",
    "\n",
    "as a Python function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Implement the signal as a function\n",
    "###\n",
    "\n",
    "# NOTE: No need to edit this variable list\n",
    "var_signal = [\"t\", \"C\", \"L\", \"V0\"]\n",
    "\n",
    "def fun_signal(t, C, L, V0):\n",
    "    # TODO: Implement the signal function\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: No need to edit; use this to check your work\n",
    "F = gr.Function(\n",
    "    fun_signal,\n",
    "    var_signal,\n",
    "    [\"i\"],\n",
    "    \"Signal\",\n",
    "    0,\n",
    ")\n",
    "df_res = F.eval(gr.df_make(t=np.pi, C=0.25, L=1, V0=1))\n",
    "assert(abs(df_res.i[0]) < 1e-15)\n",
    "\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51898fbe",
   "metadata": {},
   "source": [
    "Grama contains a routine `fit_nls()` that performs NLS automatically: It takes a dataset and a parameterized model, formulates the MSE objective, and runs `minimize` to find an optimal parameter value. To use `fit_nls()`, you must create a Grama model.\n",
    "\n",
    "### q4\\.2 Implement the signal as a Grama model\n",
    "\n",
    "Add your function to the following Grama model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Add the function you implemented above to the following model\n",
    "###\n",
    "\n",
    "# NOTE: No need to edit\n",
    "md_signal = (\n",
    "    gr.Model(\"Signal\")\n",
    "## TODO: Add your function from q3.1 here\n",
    "#    >> ???\n",
    "\n",
    "## NOTE: No need to change these bounds\n",
    "    >> gr.cp_bounds(\n",
    "        V0=(-10, 40),\n",
    "        C=(0.1, 1),\n",
    "        L=(0.5, 1.5),\n",
    "    )\n",
    ")\n",
    "\n",
    "## NOTE: No need to edit; this will run NLS on your Grama model\n",
    "md_fit1 = gr.fit_nls(\n",
    "    df_data,\n",
    "    md=md_signal,\n",
    "    uq_method=\"linpool\",\n",
    "    seed=101,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5904f",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- What does the `success` column of the output indicate? What does this tell you about the optimization of the squared-error? Does this guarantee the `mse` reached the *global* minimum?\n",
    "  - (Your response here)\n",
    "- The output reports both the initial guess parameters (e.g. `V0_0`) and their optimized values (e.g. `V0`). Inspect these values and compare them against the original data. Are these values reasonable?\n",
    "  - (Your response here)\n",
    "- Compare the initial guess parameter values to the bounds set in `cp_bounds()` above; how are they related?\n",
    "  - (Your response here)\n",
    "- Try commenting out the bounds in the model definition above. What happens?\n",
    "  - *Note*: Comment syntax in Python uses a hash symbol `#`\n",
    "  - (Your response here)\n",
    "- Do you get a `Warning` from `fit_nls()`? What does this warning say?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8968f",
   "metadata": {},
   "source": [
    "The `fit_nls()` routine returns a new Grama model with the parameters \"frozen\" to the optimal value. Thus you can evaluate this new model to make predictions.\n",
    "\n",
    "It is good practice when performing NLS to not only check the MSE, but to also visually inspect the fit.\n",
    "\n",
    "### q4\\.3 Visually inspect the fit\n",
    "\n",
    "Use the Grama verb `eval_nominal()` to evaluate the fitted model `md_fit1` at the time points in the dataset `df_data`. Answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111011a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Evaluate the fitted model at its nominal conditions\n",
    "###\n",
    "\n",
    "## TASK: Use gr.eval_nominal() to evaluate md_fit1\n",
    "##       at the given time points df_data\n",
    "# df_res1 = ???\n",
    "\n",
    "\n",
    "# NOTE: No need to edit, this will visualize your results\n",
    "(\n",
    "    df_data\n",
    "    >> gr.ggplot(gr.aes(\"t\", \"i\"))\n",
    "    + gr.geom_line(data=df_res1)\n",
    "    + gr.geom_point()\n",
    "    + gr.labs(x=\"Time (s)\", y=\"Current (Amps)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211c245",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- **Qualitatively**, how does your `Fitted` signal compare with the `Measured` signal: well or poorly?\n",
    "- What **specific features of the `Fitted` signal** do not match?\n",
    "- Did the fitting with `fit_nls()` above have `success == True`? In what sense is a result with `success == True` optimal? What does this tell you about the relationship between a \"successful\" optimization of the squared-error and the quality of a fit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9881ec3",
   "metadata": {},
   "source": [
    "As we saw in Section 2 above, *multiple starting points* can help us overcome issues of local minima. The `fit_nls()` routine has a keyword argument to automatically use a user-specified number of restarts, drawn randomly from the model bounds.\n",
    "\n",
    "### q4\\.4 Use multiple restarts\n",
    "\n",
    "Consult the documentation for `fit_nls()` (remember `Shift + Tab`) and re-run `fit_nls()` with multiple restarts. Answer the questions below, then move on to the next code chunk to visually inspect your fit.\n",
    "\n",
    "*Hint*: Using an appropriate number of restarts, it is possible to get a *very accurate* fit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# TASK: Re-fit the model with fit_nls() using multiple restarts\n",
    "# HINT: Read the documentation for fit_nls() to find a keyword argument\n",
    "#       to control the number of restarts.\n",
    "###\n",
    "\n",
    "## TASK: Re-fit the model using at least 5 restarts\n",
    "# md_fit2 = ???\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567197a",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- How does the `fit_nls()` report differ from the results in q3?\n",
    "  - (Your response here)\n",
    "- How do the optimized MSE values compare across the multiple restarts? Which set of optimized parameters do you think `fit_nls()` returns?\n",
    "  - (Your response here)\n",
    "- Do you see multiple distinct restarts arrive at the same minimizing parameter value? What does this indicate about the fit?\n",
    "  - (Your response here)\n",
    "- Do you still get a `Warning` from `fit_nls()`? (yes or no)\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Visually inspect the results of your new fit\n",
    "###\n",
    "\n",
    "## TASK: Evaluate your model at its nominal conditions, \n",
    "##       visualize the fit against the measured data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc0b81",
   "metadata": {},
   "source": [
    "### Unidentifiability and Local Minima\n",
    "\n",
    "The `fit_nls()` routine keeps warning us that the model is \"locally unidentifiable\"; what does this mean? To explain, let's evaluate the circuit model at two *completely different* parameter vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc949b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit; run and inspect\n",
    "df_test1 = gr.eval_df(\n",
    "    md_signal,\n",
    "    df=gr.tran_outer(df_data, gr.df_make(V0=20, L=0.5, C=0.5))\n",
    ")\n",
    "# NOTE: The following evaluates the signal model \n",
    "#       at **distinct** parameter values\n",
    "df_test2 = gr.eval_df(\n",
    "    md_signal,\n",
    "    df=gr.tran_outer(df_data, gr.df_make(V0=40, L=1.0, C=0.25))\n",
    ")\n",
    "\n",
    "# Visualize both evaluations of the Signal model\n",
    "# plt.figure()\n",
    "# plt.plot(df_test1.t, df_test1.i, 'b-', label=\"Set 1\")\n",
    "# plt.plot(df_test2.t, df_test2.i, 'r--', label=\"Set 2\")\n",
    "\n",
    "# plt.xlabel(\"Time (s)\")\n",
    "# plt.ylabel(\"Current (Amps)\")\n",
    "# plt.legend(loc=0)\n",
    "\n",
    "(\n",
    "    gr.ggplot(gr.aes(\"t\", \"i\"))\n",
    "    + gr.geom_line(data=df_test1, color=\"red\", size=2)\n",
    "    + gr.geom_line(data=df_test2, color=\"blue\", size=1, linetype=\"dashed\")\n",
    "    + gr.labs(x=\"Time (s)\", y=\"Current (Amps)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088ecd9",
   "metadata": {},
   "source": [
    "These two models lie **exactly** on top of each other, despite having different parameter values! This is what it means for a model to be [unidentifiable](https://en.wikipedia.org/wiki/Identifiability)---we cannot distinguish between different parameter values.\n",
    "\n",
    "There are various ways to solve issues of identifiability; one way is to use some independent source of information to *fix* one of the parameters and leave it out of the fitting procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a2f19",
   "metadata": {},
   "source": [
    "### q4\\.5 Fix a parameter\n",
    "\n",
    "Recall that $V_0$ is the initial voltage difference across the circuit. Suppose we know that `V0 == 20` Volts for the experiment that generated our data. Override the bounds of the model to constrain the value of `V0` to `20`, re-fit the new model, and answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5fff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Override the bounds of the waveform model to fix V0\n",
    "###\n",
    "\n",
    "## TASK: Freeze the V0 parameter by setting new bounds\n",
    "# md_freeze = ???\n",
    "## TASK: Fit the new \"frozen\" model\n",
    "# md_fit3 = gr.fit_nls(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a97f49",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- With this \"frozen\" model, do you see multiple distinct restarts arrive at the same minimizer? What does this indicate about the optimization of the squared-error? Does this *guarantee* that your minimizer is the global minimizer?\n",
    "  - (Your response here)\n",
    "  - (Assess your evidence here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e2d3a",
   "metadata": {},
   "source": [
    "## S5: Project Tasks\n",
    "\n",
    "---\n",
    "\n",
    "Look back on your project idea from the previous assignment. As we saw in this assignment, local minima can stymie optimization success. This project task will help you connect these ideas to your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8603cd",
   "metadata": {},
   "source": [
    "### q5\\.1 Model\n",
    "\n",
    "*Task*: Draw your first tenchi diagram. This must include:\n",
    "- A visual depiction of the natural world. Note: This cannot just be words in circles, you must use images (clipart or stick figures are fine).\n",
    "  - Include a system boundary\n",
    "  - Ensure that there are things inside and outside the system boundary\n",
    "- A depiction of the model\n",
    "\n",
    "(Your tenchi diagram here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df4dd8",
   "metadata": {},
   "source": [
    "### q5\\.2 Math\n",
    "\n",
    "*Task*: Provide an answer to each point below:\n",
    "\n",
    "- *Optimization objective*: What objective(s) are you seeking to optimize? How well can you formulate those mathematically? *Hint*: Your tenchi diagram should help you do this: The thing you're optimizing should be clearly depicted inside the system boundary.\n",
    "  - (Your answer here)\n",
    "- *Optimization variables*: What variables will you optimize your objective with respect to? (the `wrt` line in your optimization.) *Hint*: Your tenchi diagram should help you do this: The variables you're considering should be clearly represented by something inside the system boundary.\n",
    "  - (Your answer here)\n",
    "  \n",
    "- *Sources of local minima*: Are there potential sources of local minima in your project?\n",
    "  - (Your answer here)\n",
    "  - *Note*: You can often use discipline-specific knowledge to help identify potential sources of local minima. As we saw above `L, C` combine to define the natural frequency of a circuit $\\omega_0$. If your problem depends on *products* or *ratios* of physical quantities, then this is a likely mechanism for local minima in your problem.\n",
    "- *Potential reformulation of your problem*: If you identified sources of local minima above, can you reformulate your problem to avoid these issues?\n",
    "  - (Your answer here)\n",
    "  - *Note*: One way to reformulate a model is to re-express the model in terms of a smaller parameter set. For example if two geometries enter into a problem `L, W` only as an aspect ratio `AR = L/W`, then you can replace both `L, W` with `AR` alone. \n",
    "  - *Note*: If you can *fix* one of the variables in your optimization (as we did with the NLS case study above), this is another way to \"solve\" issues of local minima.\n",
    "- *Computational tractability of multiple restarts*: Will it be *computationally tractable* to run multiple restarts of your model? \n",
    "  - (Your answer here)\n",
    "  - *Note*: If you are using an expensive simulation (e.g. FEA) multiple restarts may not be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e128ec",
   "metadata": {},
   "source": [
    "### q5\\.3 People\n",
    "\n",
    "*Task*: Analyze your problem through a lens of (technomoral) humility: List at least *two things* your model can't account for, and what issues (for you, for users, for society...) those might pose.\n",
    "\n",
    "*Hint*: Your tenchi diagram should help you do this. In particular, things that are *outside* the system boundary are things that you *cannot account for* in your optimization. Clearly, these can raise issues.\n",
    "\n",
    "- (Your first thing)\n",
    "  - (What issue(s) that might raise)\n",
    "- (Your second thing)\n",
    "  - (What issue(s) that might raise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
